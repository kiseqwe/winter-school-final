import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# –í–∏–º–∏–∫–∞—î–º–æ GPU (–±–æ –º–∏ –Ω–∞ CPU)
os.environ["CUDA_VISIBLE_DEVICES"] = ""

def main():
    print("‚öôÔ∏è Loading model for inference on CPU...")
    
    BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    
    # –í–∏–∑–Ω–∞—á–∞—î–º–æ —à–ª—è—Ö –¥–æ –∞–¥–∞–ø—Ç–µ—Ä—ñ–≤ –≤—ñ–¥–Ω–æ—Å–Ω–æ —Å–∫—Ä–∏–ø—Ç–∞
    # (—à—É–∫–∞—î–º–æ –ø–∞–ø–∫—É taxi_dpo_cpu_final –ø–æ—Ä—É—á —ñ–∑ —Ü–∏–º —Ñ–∞–π–ª–æ–º)
    script_dir = os.path.dirname(os.path.abspath(__file__))
    ADAPTER_PATH = os.path.join(script_dir, "taxi_dpo_cpu_final")

    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ë–ê–ó–û–í–£ –º–æ–¥–µ–ª—å
    print(f"‚è≥ Loading base model: {BASE_MODEL}")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float32,
        device_map="cpu",
        low_cpu_mem_usage=True
    )

    # 2. –ù–∞–¥—è–≥–∞—î–º–æ –Ω–∞ –Ω–µ—ó —Ç–≤–æ—ó –ê–î–ê–ü–¢–ï–†–ò
    print(f"üîó Loading adapters from {ADAPTER_PATH}...")
    try:
        model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
        print("‚úÖ Adapters loaded successfully!")
    except Exception as e:
        print(f"‚ùå Error loading adapters: {e}")
        print("–°–ø–µ—Ä—à—É –∑–∞–ø—É—Å—Ç–∏ train_dpo.py, —â–æ–± —Å—Ç–≤–æ—Ä–∏—Ç–∏ –º–æ–¥–µ–ª—å!")
        return

    # 3. –¢–µ—Å—Ç–æ–≤–∏–π –ø—Ä–æ–º–ø—Ç
    prompt_text = "–Ø –ø–ª–∞–Ω—É—é –ø–æ—ó–∑–¥–∫—É –≤ NYC. –î–µ–Ω—å: Friday, —á–∞—Å: 18:00. –ü–∞—Å–∞–∂–∏—Ä—ñ–≤: 2. –°–∫—ñ–ª—å–∫–∏ —Ü–µ –∑–∞–π–º–µ —á–∞—Å—É?"
    
    # –§–æ—Ä–º–∞—Ç ChatML
    messages = [{"role": "user", "content": prompt_text}]
    
    input_ids = tokenizer.apply_chat_template(
        messages, 
        return_tensors="pt", 
        add_generation_prompt=True
    ).to("cpu")

    print("ü§ñ Generating response... (Please wait)")
    
    with torch.no_grad():
        outputs = model.generate(
            input_ids, 
            max_new_tokens=100, 
            do_sample=True, 
            temperature=0.7, 
            top_k=50, 
            top_p=0.95
        )

    # –î–µ–∫–æ–¥—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print("\n" + "="*30)
    print("üó£Ô∏è MODEL RESPONSE:")
    print("="*30)
    # –ü–æ–∫–∞–∑—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∞—Å–∏—Å—Ç–µ–Ω—Ç–∞
    if "assistant" in response:
        print(response.split("assistant")[-1].strip())
    else:
        print(response)
    print("="*30)

if __name__ == "__main__":
    main()