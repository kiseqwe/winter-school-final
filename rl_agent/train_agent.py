import numpy as np
import random
import time
import os

# --- –ó–ú–Ü–ù–ò –¢–£–¢: –ü–†–ò–ë–ò–†–ê–Ñ–ú–û TRY/EXCEPT ---
from taxi_game import TaxiEnv 
# ----------------------------------------

# --- 1. –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø (Hyperparameters) ---
EPISODES = 5000        # –°–∫—ñ–ª—å–∫–∏ —ñ–≥–æ—Ä –∑—ñ–≥—Ä–∞—î –∞–≥–µ–Ω—Ç –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è
MAX_STEPS = 99         # –ú–∞–∫—Å–∏–º—É–º –∫—Ä–æ–∫—ñ–≤ –∑–∞ –≥—Ä—É (—â–æ–± –Ω–µ –∑–∞—Ü–∏–∫–ª–∏–≤—Å—è)

LEARNING_RATE = 0.1    # Alpha: –Ω–∞—Å–∫—ñ–ª—å–∫–∏ —à–≤–∏–¥–∫–æ –∞–≥–µ–Ω—Ç –∑–∞–±—É–≤–∞—î —Å—Ç–∞—Ä–µ —ñ –≤—á–∏—Ç—å –Ω–æ–≤–µ
DISCOUNT_RATE = 0.9    # Gamma: –Ω–∞—Å–∫—ñ–ª—å–∫–∏ –≤–∞–∂–ª–∏–≤–∞ –º–∞–π–±—É—Ç–Ω—è –Ω–∞–≥–æ—Ä–æ–¥–∞ (0.9 = –¥—É–∂–µ –≤–∞–∂–ª–∏–≤–∞)

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è (Exploration vs Exploitation)
EPSILON = 1.0          # –®–∞–Ω—Å –∑—Ä–æ–±–∏—Ç–∏ –≤–∏–ø–∞–¥–∫–æ–≤–∏–π —Ö—ñ–¥ (—Å–ø–æ—á–∞—Ç–∫—É 100%)
EPSILON_DECAY = 0.999  # –ó–º–µ–Ω—à—É—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤—ñ—Å—Ç—å –∑ –∫–æ–∂–Ω–æ—é –≥—Ä–æ—é
EPSILON_MIN = 0.01     # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π —à–∞–Ω—Å –≤–∏–ø–∞–¥–∫–æ–≤–æ—Å—Ç—ñ (1%)

# --- 2. –Ü–ù–Ü–¶–Ü–ê–õ–Ü–ó–ê–¶–Ü–Ø ---
env = TaxiEnv()

# –†–æ–∑–º—ñ—Ä Q-Table: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å—Ç–∞–Ω—ñ–≤ x –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥—ñ–π
# –£ –Ω–∞—à—ñ–π –≥—Ä—ñ 500 —Å—Ç–∞–Ω—ñ–≤ —ñ 6 –¥—ñ–π
q_table = np.zeros((500, 6))

print("üöñ Start Training (Q-Learning)...")

# --- 3. –¢–†–ï–ù–£–í–ê–õ–¨–ù–ò–ô –¶–ò–ö–õ ---
for episode in range(EPISODES):
    state = env.reset()
    done = False
    
    for step in range(MAX_STEPS):
        # A. –í–∏–±—ñ—Ä –¥—ñ—ó (Epsilon-Greedy Strategy)
        if random.uniform(0, 1) < EPSILON:
            action = random.randint(0, 5)  # Exploration: –í–∏–ø–∞–¥–∫–æ–≤–∏–π —Ç–∏—Ü—å
        else:
            action = np.argmax(q_table[state]) # Exploitation: –ù–∞–π–∫—Ä–∞—â–∏–π –≤—ñ–¥–æ–º–∏–π —Ö—ñ–¥

        # B. –í–∏–∫–æ–Ω—É—î–º–æ –¥—ñ—é
        next_state, reward, done = env.step(action)

        # C. –û–Ω–æ–≤–ª—é—î–º–æ Q-Table (–§–æ—Ä–º—É–ª–∞ –ë–µ–ª–ª–º–∞–Ω–∞)
        old_value = q_table[state, action]
        next_max = np.max(q_table[next_state])
        
        # Q(s,a) = (1-lr)*Q(s,a) + lr*(reward + gamma*maxQ(s',a'))
        new_value = (1 - LEARNING_RATE) * old_value + LEARNING_RATE * (reward + DISCOUNT_RATE * next_max)
        q_table[state, action] = new_value

        state = next_state

        if done:
            break
            
    # –ó–º–µ–Ω—à—É—î–º–æ Epsilon (–∞–≥–µ–Ω—Ç —Å—Ç–∞—î –≤–ø–µ–≤–Ω–µ–Ω—ñ—à–∏–º)
    if EPSILON > EPSILON_MIN:
        EPSILON *= EPSILON_DECAY

    # –õ–æ–≥—É–≤–∞–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É
    if (episode + 1) % 500 == 0:
        print(f"Episode: {episode + 1} | Epsilon: {EPSILON:.4f}")

print("‚úÖ Training Finished!\n")

# --- 4. DEMO (SHOWTIME) ---
# –ó–∞—Ä–∞–∑ –º–∏ –ø–æ–∫–∞–∂–µ–º–æ, —è–∫ –≥—Ä–∞—î –≤–∂–µ –Ω–∞–≤—á–µ–Ω–∏–π –∞–≥–µ–Ω—Ç
input("–ù–∞—Ç–∏—Å–Ω–∏ Enter, —â–æ–± –ø–æ–¥–∏–≤–∏—Ç–∏—Å—è –¥–µ–º–æ-–≥—Ä—É –Ω–∞–≤—á–µ–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞...")
os.system('cls' if os.name == 'nt' else 'clear') # –û—á–∏—Å—Ç–∏—Ç–∏ –∫–æ–Ω—Å–æ–ª—å

state = env.reset()
done = False
total_reward = 0
actions_map = ["South üëá", "North üëÜ", "East üëâ", "West üëà", "PICKUP üéí", "DROPOFF üèÅ"]

print("*** üöñ SMART TAXI DEMO ***")

for step in range(25):
    # –¢—ñ–ª—å–∫–∏ Exploitation (–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–Ω–∞–Ω–Ω—è)
    action = np.argmax(q_table[state])
    
    # –í–∏–∫–æ–Ω—É—î–º–æ –¥—ñ—é
    next_state, reward, done = env.step(action)
    
    print(f"Step {step+1}: {actions_map[action]} (Reward: {reward})")
    
    total_reward += reward
    state = next_state
    
    # –ú–∞–ª–µ–Ω—å–∫–∞ –ø–∞—É–∑–∞ –¥–ª—è –µ—Ñ–µ–∫—Ç—É –∫—ñ–Ω–æ
    time.sleep(0.5) 
    
    if done:
        print(f"\nüèÜ SUCCESS! Total Score: {total_reward}")
        break

if not done:
    print("\n‚ö†Ô∏è Failed to complete in 25 steps.")